import torch


t0 = torch.tensor(3)
# o-d tensor == scalar


t0.shape


t1 = torch.tensor([1,2,3])


t2 = torch.tensor([[1,2,3], [4,5,6]])


x = torch.tensor([1,2,3])
y = x.type(torch.float)


y


y.type()


x = torch.tensor([1., 2., 3.])
y = x.type(torch.int)


y.type()


t2


x = torch.tensor([1., 2., 3.])
y = torch.tensor([4., 5., 6.])
x+y


x.add(y)


x.add_(y)


# add_ is in place 
# add( ) returns the added value
# + is an operator



x = torch.tensor([1., 2., 3.])


# Sum and extract values
s = x.sum()
# s is a scalar vaule, saved as a tensor

s.item()


 x = torch.tensor([1., 2., 3.,])
x.mean()


x = torch.tensor([1,2,3])
x.mean()


try:
    x.mean()
except Exception as E:
    print(E)


A = torch.tensor(
    [[1., 2., 3.],
    [4., 5., 6.]]
)
A


A.sum()


A.sum(dim = 0)



A.sum(dim = 1)


x = torch.arange(10)
A = x.reshape(-1,5)
A


A = torch.arange(15).reshape(5,3)
A


A[2, :]


A[:, 1]


A = torch.arange(10).reshape(2,5)
b = torch.tensor([1,2,3,4,5])
A , b


A.shape, b.shape, (A@b).shape


A@b



#Vector Matrix product 
A = torch.arange(10).reshape(5,2)
b = torch.arange(5)
b, A


b @ A


b.shape, A.shape, (b@A).shape


A = torch.arange(15).reshape(5,3)
B = torch.arange(6).reshape(3,2)


A @ B


A = torch.arange(20).reshape(4,5)
b = torch.arange(5)
A + b


A = torch.arange(20).reshape(4,5)
b = torch.arange(4)


# Column vector addition
A + b.reshape(4,1)


# concatination 

A = torch.arange(6).reshape(3,2)
B = torch.arange(10).reshape(5,2)

A, B


C = torch.concatenate((A, B), dim = 0)
C.shape, C


# Flatten 
A = torch.arange(24).reshape(2,3,4)
A.shape


A[0].shape , A[0]


A


A[1].shape, A[1]


A.flatten()


A.reshape(-1)


A.reshape(2, 12)


# Zeroth dim is the batch dimension
A.flatten(start_dim=1).shape
A.flatten(start_dim=1)


x = torch.rand(5)
x


A = torch.randint(0, 5, (2,5))


A


torch.manual_seed(1001)
torch.randint(0,5, (5, ))


# Simple Linear Regression 


n = 100
x = torch.rand(n)
w_true = torch.tensor(2.)
b_true = torch.tensor(5.)
noise = torch.rand(n)

# y = 2x + 5 + noise

y = (w_true * x) + b_true + noise


import matplotlib.pyplot as plt


w = torch.tensor(0.)
b = torch.tensor(0.)

eta = 0.1
epochs = 200
losses = []
for epoch in range(epochs):
    y_hat = w * x + b
    loss = ((y_hat - y) ** 2).mean()
    losses.append(loss.item())
    w_grad = (2* (y_hat - y) * x).mean()
    b_grad = (2* (y_hat - y)).mean()
    w -= eta * w_grad
    b -= eta * w_grad

w,b
plt.plot(range(epochs), losses)




w , b


!pip install torchviz


from torchviz import make_dot
x = torch.tensor(1., requires_grad = True)
y = x**2
y.backward()
graph = make_dot(y, params = {'x':x, 'y':y})
print(x.grad)
graph


x = torch.tensor(1., requires_grad=True)
w = 2 * x   # 2
y = x ** 2  # 2 8 x 
z = w + y   # 1 ,1 
z.backward()
print(x.grad)
graph = make_dot(z , {'x':x, 'w':w, 'y':y, 'z':z})
# partial derivatives of the result of the operation with respect to the operands
graph


x = torch.tensor(1., requires_grad=True)
w = 2 * x   # 2
y = x ** 2  # 2 8 x 
z = w + y   # 1 ,1 
z.backward()
x, y , z ,w


x = torch.tensor(1., requires_grad=True)
y = x ** 2
y.backward()
x.grad


w = torch.tensor(0., requires_grad = True)
b = torch.tensor(0., requires_grad= True)
y_hat = w * x + b

eta = 0.01
epoch
loss = ((y_hat - y)**2).mean()
losses.append(loss.item())


from torch.nn import MSELoss


loss_fn = MSELoss()
inputs = torch.tensor([1.1, 2.2])
targets = torch.tensor([1., 2.])
loss_fn(inputs, targets)


# Preparing Data
n = 100
x = torch.randn(n)
w_true = torch.tensor(2.)
b_true = torch.tensor(5.)
noise = torch.randn(n)
# y = 2x + 5 + noise
y = (w_true * x + b_true) + noise
x.shape, y.shape


w = torch.tensor(0., requires_grad =True)
b = torch.tensor(0., requires_grad=True)
y_hat = w * x + b
loss = ((y_hat - y) ** 2).mean()
graph = make_dot(loss, {'w':w, 'b':b, 'loss':loss})
graph


w = torch.tensor(0., requires_grad =True)
b = torch.tensor(0., requires_grad=True)

eta = 0.1
epochs = 200
losses = []
for epoch in range(epochs):
    y_hat = w * x + b
    loss = ((y_hat - y) ** 2).mean()
    losses.append(loss.item())

    loss.backward()
    loss.

    with torch.no_grad():
        w_grad = (2* (y_hat - y) * x).mean()
        b_grad = (2* (y_hat - y)).mean()
        w -= eta * w.grad
        b -= eta * w.grad

w,b
plt.plot(range(epochs), losses)



loss_fn = MSELoss()
# loss_fn is an object of class MSELoss
# loss_fn(inputs, targets)
inputs = torch.tensor([1., 2.])
targets = torch.tensor([1.1, 2.2])
loss_fn(inputs, targets).item()


# Train model
w = torch.tensor(0., requires_grad=True)
b = torch.tensor(0., requires_grad=True)
eta = 0.01
epochs = 200
losses = [ ]

for epoch in range(epochs):
    # Forward
    y_hat = w * x + b
    loss = loss_fn(y_hat, y)
    losses.append(loss.item())

    # Backward
    loss.backward()

    # context manager
    with torch.no_grad():
        # do not track gradients
        # do not disturb the computational graph
        w -= eta * w.grad
        b -= eta * b.grad

    w.grad.zero_()
    b.grad.zero_()

print(w, b)
plt.plot(range(epochs), losses);


from torchvision import datasets


from torchvision.transforms import ToTensor


train_data = datasets.MNIST(
    root='data',
    download=True,
    train=True
)
test_data = datasets.MNIST(
    root='data',
    download=True,
    train=False
)



test_data


train_data.data.shape


len(train_data), len(test_data)





# Subscritable or indexable
img, label = train_data[0]
img, label


train_data.targets == 7


from torchvision.transforms import ToTensor



train_data = datasets.MNIST(
    root='data',
    train=True,
    transform=ToTensor()
)
img, label = train_data[0]
img, label


train_data.data.shape


train_data.targets.shape


x = torch.tensor([0, 1, 2, 3, 4])
y = torch.tensor([True, False, False, False, True])
x[y]


sevens = train_data.data[train_data.targets == 7, :, :]
sevens.shape
plt.imshow(sevens[15], cmap='gray')


torch.Size([1, 28, 28])


img.max(), img.min()


plt.imshow(img.squeeze(), cmap='gray');


x = torch.arange(10).reshape(1, 2, 5, 1)
x.squeeze().shape












